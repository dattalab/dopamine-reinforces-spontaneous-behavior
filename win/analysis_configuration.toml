# where data is stored
# reinforcement --> standard closed loop experiments (250 ms CW stim)
# reinforcement_new_syllables --> closed loop experiments testing syllables other than the "canonical 6"
# reinforcement_scalar --> yttri and dudman style experiment (stim during specific variation of syllable)
# delay_var_excite -->
# excitation --> standard closed loop experiments with 2s CW stim
# excitation_pulsed --> standard closed loop experiments with 3s pulsed stim
# open_loop --> non-yoked stim
# parameter_scan --> closed loop experiments varying power
# delay --> closed loop experiments with delayed stim
# lesions --> closed loop experiments after lesioning DLS or NAc
# XX_photometry --> experiment with simultaneous dLight/gcamp photometry
# photometry --> OFA photometry
[data_dirs]
root = "/home/markowitzmeister_gmail_com/jeff_win_share/reinforcement_data" # root directory
root_wg = "/home/wg41/reinforcement_data"
reinforcement = [
	"_aggregate_results_arhmm_03",
	"_aggregate_results_arhmm_04",
	"_aggregate_results_arhmm_05",
	"_aggregate_results_arhmm_06",
	"_aggregate_results_arhmm_07",
	"_aggregate_results_arhmm_08",
	"_aggregate_results_arhmm_09",
	"_aggregate_results_arhmm_10",
	"_aggregate_results_arhmm_11",
]
reinforcement_photometry = [
	"_aggregate_results_arhmm_photometry_02/",
	"_aggregate_results_arhmm_photometry_03/"
]
reinforcement_new_syllables = [
	"_aggregate_results_arhmm_new_syllables_01/",
	"_aggregate_results_arhmm_new_syllables_02/",
	"_aggregate_results_arhmm_new_syllables_03/"
]
reinforcement_photometry_new_syllables = [
	"_aggregate_results_arhmm_photometry_new_syllables_01/",
	"_aggregate_results_arhmm_photometry_new_syllables_02/",
	"_aggregate_results_arhmm_photometry_new_syllables_03/"
]
reinforcement_scalar = [
	"_aggregate_results_arhmm_scalar_01/",
	"_aggregate_results_arhmm_scalar_03/",
]
delay = [
	"_aggregate_results_arhmm_delays_01",
	"_aggregate_results_arhmm_delays_02",
	"_aggregate_results_arhmm_delays_03"
]
lesions = [
    "_aggregate_results_arhmm_lesions_01",
	"_aggregate_results_arhmm_lesions_02",
    "_aggregate_results_arhmm_lesions_03",
    "_aggregate_results_arhmm_lesions_04"
]
inhibition = [
	"_aggregate_results_arhmm_inhibition_01",
	"_aggregate_results_arhmm_inhibition_02",
	"_aggregate_results_arhmm_inhibition_03"
]
delay_var_excite = [
	"_aggregate_results_arhmm_delays_var_excite_01"
]
parameter_scan = [
	"_aggregate_results_arhmm_scan_01/",
	"_aggregate_results_arhmm_scan_02/"
]
excitation = [
	"_aggregate_results_arhmm_excitation_01/",
	"_aggregate_results_arhmm_excitation_02/",
	"_aggregate_results_arhmm_excitation_03/",
	"_aggregate_results_arhmm_excitation_04/",
]
excitation_photometry = [
	"_aggregate_results_arhmm_photometry_excitation_02/",
]

excitation_pulsed = [
	"_aggregate_results_arhmm_excitation_pulsed_01/"
]

excitation_pulsed_photometry = [
	"_aggregate_results_arhmm_photometry_excitation_pulsed_01/"
]
open_loop = [
	"_results_open_loop_01/_aggregate_results/"
]
photometry = [
	"_aggregate_results_arhmm_photometry_06/",
	"_aggregate_results_arhmm_photometry_07/"
]

[intermediate_results]
dlight_public = "/n/groups/datta/win/reinforcement-data/public-dataset/dlight_photometry_processed_full.parquet"
features_public = "/n/groups/datta/win/reinforcement-data/public-dataset/dlight-chrimson_snippets_offline_features.parquet"
dlight = "/home/markowitzmeister_gmail_com/jeff_win_share/reinforcement_data/_proc_photometry_dlight/"
closed_loop_behavior = "/home/markowitzmeister_gmail_com/jeff_win_share/reinforcement_data/_proc_closed_loop_behavior/"
rl_modeling = "/home/markowitzmeister_gmail_com/jeff_win_share/reinforcement_data/_proc_rl_modeling/"

# remember to append this to `root_wg`
dlight_wg = "_proc_photometry_dlight/dlight_photometry_processed_full.parquet"
dlight_snippet_features_wg.offline = "_proc_photometry_dlight/dlight_snippets_offline_features.parquet"
dlight_snippet_features_wg.online = "_proc_photometry_dlight/dlight_snippets_online_features.parquet"
dlight_syllable_stats.offline = "_proc_photometry_dlight/syllable_stats_photometry_offline.toml"
dlight_syllable_stats.online = "_proc_photometry_dlight/syllable_stats_photometry_online.toml"


[figures]
store_dir = "/home/markowitzmeister_gmail_com/panels/2022/2022 (DA paper, first submission)"
dir_wg = "/home/wg41/da-paper-figures/submission-2022"

[common]
fs = 30.0

[dlight_common]
dff_threshold = 1.5
dlight_reference_corr_threshold = 0.6
raw_dff_threshold = 1  # i.e., 100% df/f0 - typlcal 99%ile range is -1% to 8%

[dlight_snippet]
dataframe_filename = "_jeff-cache/merged_feedback_dataframe_debounce11.parquet"
scalars.hampel_filter.threshold = 10.0
reference.filter.corner_fs = 3.0
reference.filter.order = 2
rolling_z.window = 600
rolling_z.min_periods = 30
rolling_z.center = true
snippet_grab.window_bounds = [-3, 3]
snippet_grab.gb_key = "uuid"
# snippet_grab.label_key = "predicted_syllable"
# snippet_grab.label_key = "predicted_syllable (offline)"
snippet_grab.label_key = "movement_initiations"
data_keys = [
    "signal_dff",
    "reference_dff",
    "reference_dff_fit",
    "signal_reref_dff",
    "velocity_2d_mm",
    "velocity_3d_mm",
    "height_ave_mm",
    "width_mm",
    "length_mm",
    "cp_score",
	"feedback_status",
    "velocity_3d_head_mm",
    "velocity_3d_tail_mm",
    "velocity_3d_middle_mm",
    "acceleration_2d_mm",
    "acceleration_3d_mm",
	"jerk_2d_mm",
	"jerk_3d_mm",
    "angle",
    "angle_unwrapped",
    "velocity_angle",
    "velocity_height",
	"pc00",
	"pc01",
	"pc02",
    "pc03",
	"pc04",
	"pc05",
	"pc06",
	"pc07",
	"pc08",
	"pc09",
    "timestamp",
]
meta_keys = [
	"uuid",
	"mouse_id",
	"session_number",
	"target_syllable",
	"stim_duration",
	"date",
	"trial_count",
	"opsin",
	"area"]
convs.target_syllable = "int16"
convs.session_number = "int8"
convs.signal_reference_corr = "float32"
convs.signal_max = "float32"
convs.reference_max = "float32"
convs.uuid = "category"
convs.mouse_id = "category"
convs.area = "category"

[dlight_basic_analysis]
scalars = [
	"velocity_2d_mm",
	"acceleration_2d_mm",
	"jerk_2d_mm",
	"velocity_angle",
	"velocity_height"
	]
timescale_correlation.bins = [0.25, 60.1, 0.5]
timescale_correlation.nshuffles = 1000
dlight_key = "signal_reref_dff_z"

[dlight_regression]
scalars = [
	"velocity_2d_mm",
	"acceleration_2d_mm",
	"velocity_angle",
	"velocity_height"
	]

[dlight_transition_features]
use_offline = true
renormalize = false
pre_window = [-0.5, 5.0]
windows = [[-0.2, 0.3],[-0.1, 0.4],[0.0, 0.5],[0.0, 0.3],[0.0,0.6],[0.0,1.0]]
# scalar included along with the features
scalars = [
	"velocity_2d_mm",
	"acceleration_2d_mm",
	"jerk_2d_mm",
	"velocity_angle",
	"velocity_height"
	]
# metadata included in the features
meta_keys = [
	"uuid",
	"timestamp",
	"syllable",
	"mouse_id",
	"session_number",
	"stim_duration",
	"duration",
	"target_syllable",
	"signal_max",
	"reference_max",
	"signal_reference_corr",
	"date",
	"area",
	"opsin",
	]
# channels to compute features over
proc_keys = [
	"signal_dff_z",
	"signal_reref_dff_z",
	"reference_dff_z"
]

[dlight_encoding_features]
label_key = "predicted_syllable (offline)"
# window_sizes = [5, 15, 30, 60, 120, 240, 480, 960, 1920]
window_sizes = [5, 10, 25, 50, 100, 200, 300, 400, 800, 1600]
average_variant.bins = [5, 10, 25, 50, 100, 200, 300, 400, 800, 1600]
# average_variant.bins = [10,25,50,100,200,300,400,800,1600]
# average_variant.bins = [10, 25, 50, 100, 200, 400, 800]

[dlight_lagged_correlations]
nshuffles = 1000
use_offline = true
use_renormalized = false
estimate_within_bin = true
use_neural_features = [
	"signal_reref_dff_z_max",
	"reference_dff_z_max",
	"signal_dff_z_max",
]
use_windows = ["(0.0, 0.3)"]
correlation_method = "pandas"
correlation_kwargs.method = "pearson"
usage_and_scalars.bins = [10, 400, 20]
usage_and_scalars.scalars = [
	"velocity_2d_mm",
	# "velocity_height",
	# "velocity_angle",
	# "acceleration_2d_mm",
	# "duration",
]
# usage_and_scalars.correlation_keys = ["syllable", "uuid", "bin"] # compute correlation *within* these groups
usage_and_scalars.correlation_keys = ["syllable", "mouse_id", "bin"]
entropy.bins = [5, 52, 5]
entropy.ndlight_bins  = 10 # bin dlight then aggregate tm stats
entropy.dlight_bin_keys = ["mouse_id", "syllable"] # compute dlight bin cutoffs over these keys
entropy.pre_agg_keys = ["uuid", "mouse_id", "bin", "dlight_bin_feature", "dlight_bin"] # this first aggregate just reduces data size
entropy.agg_keys = ["mouse_id", "bin", "dlight_bin_feature", "dlight_bin"] # compute entropy with these keys, last key is chopped off and used in correlation
entropy.correlation_keys = ["dlight_bin_feature", "bin"] # final groupby, then correlation is averaged
entropy.corr_kwargs.method = "spearman"
entropy.tm_truncate = 36
clustering.nclusters = 4
clustering.bin_size = 60
clustering.use_features = [
	"velocity_2d_mm",
    "signal_reref_dff_z_max",
]
usage_and_scalars_shifted.bins = [10]
stim.use_windows = ["(0.0, 0.6)", "(0.0, 1.0)"]
stim.estimate_within_bin = true



[closed_loop_behavior]
dataframe_filename = "_jeff-cache/feedback_dataframe_debounce11.parquet"
learning_timecourse.bin_size = 30
learning_timecourse.bin_overlap = 0
learning_timecourse.baseline = "m"
learning_timecourse.meta_keys = [
    "mouse_id",
    "session_number",
    "syllable_group",
    "target_syllable",
    "stim_duration",
    "area (pooled)",
    "area",
    "genotype",
    "uuid",
    "date",
    "cohort",
]

[dask]
address="tcp://10.10.0.21:35929"
#dask_address = None