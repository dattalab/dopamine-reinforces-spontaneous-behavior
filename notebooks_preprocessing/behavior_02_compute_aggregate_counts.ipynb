{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute session-level aggregate syllable counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "from rl_analysis.behavior.util import normalize_df, filter_feedback_dataframe\n",
    "from rl_analysis.batch import apply_parallel_joblib\n",
    "from rl_analysis.io.df import get_closed_loop_parquet_columns\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in raw data and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../analysis_configuration.toml\", \"r\") as f:\n",
    "    analysis_config = toml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dirs = analysis_config[\"raw_data\"]\n",
    "proc_dirs = analysis_config[\"intermediate_results\"]\n",
    "closed_loop_cfg = analysis_config[\"closed_loop_behavior\"]\n",
    "common_cfg = analysis_config[\"common\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(raw_dirs[\"closed_loop_behavior\"], \"closed_loop_behavior.parquet\")\n",
    "cols = get_closed_loop_parquet_columns(fname, pcs=False, likes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.read_parquet(\n",
    "    fname,\n",
    "    filters=[\n",
    "        (\n",
    "            \"experiment_type\",\n",
    "            \"in\",\n",
    "            [\n",
    "                \"reinforcement\",\n",
    "                \"reinforcement_photometry\",\n",
    "                \"excitation\",\n",
    "                \"excitation_photometry\",\n",
    "                \"excitation_pulsed\",\n",
    "                \"excitation_pulsed_photometry\",\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    columns=cols,\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = filter_feedback_dataframe(feedback_df, **common_cfg)\n",
    "feedback_df.index = range(len(feedback_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in raw data and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the \"normalized\" dataframe (pretty memory intensive with target_only set to False, ~50-60 GB of RAM required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df[\"timestamp\"] = feedback_df.groupby(\"uniq_id\")[\"timestamp\"].transform(\n",
    "    lambda x: (x - x.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_timestamp = 0\n",
    "last_timestamp = 30 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0  # prob smoothing, 0-1 (.01 probably the max you want to use)\n",
    "max_syllable = 100  # max syllable in the model\n",
    "target_only = False  # only keep the target? (False keeps everything)\n",
    "use_rle = True  # run-length-encode?\n",
    "baseline = \"m\"  # (a)bsolute to use the first baseline session, (m)onday for mondays, (w)eek for earliest baseline session in the past week and (l)ocal for the closest baseline\n",
    "label_key = \"predicted_syllable\"  # predicted_syllable or predicted_syllable (offline)\n",
    "time_bins = [first_timestamp, last_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   34.6s\n",
      "/home/markowitzmeister_gmail_com/dev/dopamine-reinforces-spontaneous-behavior/rl_analysis/behavior/util.py:109: UserWarning: Less than two baseline sessions for [12, 20, '3440']\n",
      "  warnings.warn(f\"Less than two baseline sessions for {print_key}\")\n",
      "/home/markowitzmeister_gmail_com/dev/dopamine-reinforces-spontaneous-behavior/rl_analysis/behavior/util.py:109: UserWarning: Less than two baseline sessions for [12, 59, '3440']\n",
      "  warnings.warn(f\"Less than two baseline sessions for {print_key}\")\n",
      "/home/markowitzmeister_gmail_com/dev/dopamine-reinforces-spontaneous-behavior/rl_analysis/behavior/util.py:109: UserWarning: Less than two baseline sessions for [12, 30, '3442']\n",
      "  warnings.warn(f\"Less than two baseline sessions for {print_key}\")\n",
      "[Parallel(n_jobs=20)]: Done   7 out of  41 | elapsed:   53.0s remaining:  4.3min\n",
      "[Parallel(n_jobs=20)]: Done  12 out of  41 | elapsed:   54.5s remaining:  2.2min\n",
      "[Parallel(n_jobs=20)]: Done  17 out of  41 | elapsed:   56.6s remaining:  1.3min\n",
      "/home/markowitzmeister_gmail_com/dev/dopamine-reinforces-spontaneous-behavior/rl_analysis/behavior/util.py:109: UserWarning: Less than two baseline sessions for [13, 30, 'dlight-chrimson-6']\n",
      "  warnings.warn(f\"Less than two baseline sessions for {print_key}\")\n",
      "[Parallel(n_jobs=20)]: Done  22 out of  41 | elapsed:   59.3s remaining:   51.2s\n",
      "[Parallel(n_jobs=20)]: Done  27 out of  41 | elapsed:  1.0min remaining:   32.3s\n",
      "[Parallel(n_jobs=20)]: Done  32 out of  41 | elapsed:  1.1min remaining:   18.1s\n",
      "[Parallel(n_jobs=20)]: Done  37 out of  41 | elapsed:  1.1min remaining:    7.3s\n",
      "[Parallel(n_jobs=20)]: Done  41 out of  41 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "# metadata to preserve\n",
    "meta_keys = [\n",
    "    \"sex\",\n",
    "    \"mouse_id\",\n",
    "    \"session_number\",\n",
    "    \"stim_duration\",\n",
    "    \"syllable_group\",\n",
    "    \"target_syllable\",\n",
    "    \"opsin\",\n",
    "    \"experiment_type\",\n",
    "    \"area (pooled)\",\n",
    "    \"power\",\n",
    "    \"area\",\n",
    "    \"genotype\",\n",
    "    \"uuid\",\n",
    "    \"date\",\n",
    "    \"cohort\",\n",
    "]\n",
    "group_dfs = []\n",
    "\n",
    "# outer loop by cohort, inner loop by target, first check all timecourses, then wed-mon\n",
    "_func = partial(\n",
    "    normalize_df,\n",
    "    label_key=label_key,\n",
    "    outer_loop_key=[\"cohort\", \"target_syllable\"],\n",
    "    time_bins=time_bins,\n",
    "    eps=eps,\n",
    "    meta_keys=meta_keys,\n",
    "    target_only=target_only,\n",
    "    use_rle=use_rle,\n",
    "    baseline=baseline,\n",
    "    adjust_to_bin_size=True,\n",
    ")\n",
    "\n",
    "norm_df = apply_parallel_joblib(\n",
    "    feedback_df.groupby(\"mouse_id\", as_index=False, group_keys=False),\n",
    "    _func,\n",
    "    n_jobs=20,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = norm_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markowitzmeister_gmail_com/miniconda3/envs/spont-da/lib/python3.10/site-packages/pandas/core/arraylike.py:405: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/markowitzmeister_gmail_com/miniconda3/envs/spont-da/lib/python3.10/site-packages/pandas/core/arraylike.py:405: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "norm_df = norm_df.replace([np.inf, -np.inf], np.nan)\n",
    "norm_df[\"log2_fold_change_count\"] = np.log2(norm_df[\"fold_change_count\"])\n",
    "norm_df[\"log2_fold_change_usage\"] = np.log2(norm_df[\"fold_change_usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df.to_parquet(\n",
    "    os.path.join(raw_dirs[\"closed_loop_behavior\"], \"learning_aggregate.parquet\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spont-da]",
   "language": "python",
   "name": "conda-env-spont-da-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
