{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute syllable counts in bins across optoda sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from rl_analysis.behavior.util import normalize_df, filter_feedback_dataframe\n",
    "from rl_analysis.batch import apply_parallel_joblib\n",
    "from rl_analysis.io.df import get_closed_loop_parquet_columns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "with open(\"../analysis_configuration.toml\", \"r\") as f:\n",
    "    analysis_config = toml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dirs = analysis_config[\"raw_data\"]\n",
    "proc_dirs = analysis_config[\"intermediate_results\"]\n",
    "closed_loop_cfg = analysis_config[\"closed_loop_behavior\"]\n",
    "common_cfg = analysis_config[\"common\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in raw data and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(raw_dirs[\"closed_loop_behavior\"], \"closed_loop_behavior.parquet\")\n",
    "cols = get_closed_loop_parquet_columns(fname, pcs=False, likes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = pd.read_parquet(\n",
    "    fname,\n",
    "    filters=[\n",
    "        (\n",
    "            \"experiment_type\",\n",
    "            \"in\",\n",
    "            [\n",
    "                \"reinforcement\",\n",
    "                \"reinforcement_photometry\",\n",
    "            ],\n",
    "        ),\n",
    "    ],\n",
    "    columns=cols,\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df = filter_feedback_dataframe(feedback_df, **common_cfg)\n",
    "feedback_df.index = range(len(feedback_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compute the \"normalized\" dataframe (pretty memory intensive with target_only set to False, ~50-60 GB of RAM required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df[\"timestamp\"] = feedback_df.groupby(\"uniq_id\")[\"timestamp\"].transform(\n",
    "    lambda x: (x - x.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_timestamp = (\n",
    "    np.ceil(feedback_df.groupby(\"uniq_id\")[\"timestamp\"].min().max() / precision)\n",
    "    * precision\n",
    ")\n",
    "last_timestamp = (\n",
    "    np.ceil(feedback_df.groupby(\"uniq_id\")[\"timestamp\"].max().min() / precision)\n",
    "    * precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_timestamp = 0\n",
    "last_timestamp = 1790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1  # only used for fold changes, pseudocount for num and den\n",
    "session_window = (first_timestamp, last_timestamp)\n",
    "bin_size = closed_loop_cfg[\"learning_timecourse\"][\"bin_size\"]\n",
    "bin_overlap = closed_loop_cfg[\"learning_timecourse\"][\n",
    "    \"bin_overlap\"\n",
    "]  # bin overlap in seconds\n",
    "max_syllable = 100  # max syllable in the model\n",
    "target_only = False  # only keep the target? (False keeps everything)\n",
    "baseline_smoothing = None\n",
    "baseline = closed_loop_cfg[\"learning_timecourse\"][\n",
    "    \"baseline\"\n",
    "]  # (a)bsolute to use the first baseline session, (m)onday for mondays, (w)eek for earliest baseline session in the past week and (l)ocal for the closets baseline\n",
    "label_key = \"predicted_syllable\"  # predicted_syllable or predicted_syllable (offline)\n",
    "time_bins = np.arange(session_window[0], session_window[1] + bin_size - 1, bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = os.path.join(\n",
    "    raw_dirs[\"closed_loop_behavior\"], f\"learning_timecourse_binsize-{bin_size}.parquet\"\n",
    ")\n",
    "syllable_list = sorted(feedback_df[\"predicted_syllable\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer loop by cohort, inner loop by target, first check all timecourses, then wed-mon\n",
    "_func_rle = partial(\n",
    "    normalize_df,\n",
    "    label_key=label_key,\n",
    "    outer_loop_key=\"syllable_group\",\n",
    "    time_bins=time_bins,\n",
    "    baseline_smoothing=baseline_smoothing,\n",
    "    eps=eps,\n",
    "    syllable_list=syllable_list,\n",
    "    meta_keys=closed_loop_cfg[\"learning_timecourse\"][\"meta_keys\"],\n",
    "    target_only=target_only,\n",
    "    use_rle=True,\n",
    "    baseline=baseline,  \n",
    ")\n",
    "\n",
    "# outer loop by cohort, inner loop by target, first check all timecourses, then wed-mon\n",
    "_func_nonrle = partial(\n",
    "    normalize_df,\n",
    "    label_key=label_key,\n",
    "    outer_loop_key=\"syllable_group\",\n",
    "    time_bins=time_bins,\n",
    "    baseline_smoothing=baseline_smoothing,\n",
    "    eps=eps,\n",
    "    syllable_list=syllable_list,\n",
    "    meta_keys=closed_loop_cfg[\"learning_timecourse\"][\"meta_keys\"],\n",
    "    target_only=target_only,\n",
    "    use_rle=False,\n",
    "    baseline=baseline,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  40 | elapsed:   30.1s remaining:  2.8min\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  40 | elapsed:   32.7s remaining:  1.4min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  40 | elapsed:   36.2s remaining:   54.2s\n",
      "[Parallel(n_jobs=20)]: Done  21 out of  40 | elapsed:   38.6s remaining:   34.9s\n",
      "[Parallel(n_jobs=20)]: Done  26 out of  40 | elapsed:   41.5s remaining:   22.4s\n",
      "[Parallel(n_jobs=20)]: Done  31 out of  40 | elapsed:   44.3s remaining:   12.9s\n",
      "[Parallel(n_jobs=20)]: Done  36 out of  40 | elapsed:   47.5s remaining:    5.3s\n",
      "[Parallel(n_jobs=20)]: Done  40 out of  40 | elapsed:   52.9s finished\n",
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=20)]: Done   6 out of  40 | elapsed:   25.2s remaining:  2.4min\n",
      "[Parallel(n_jobs=20)]: Done  11 out of  40 | elapsed:   27.9s remaining:  1.2min\n",
      "[Parallel(n_jobs=20)]: Done  16 out of  40 | elapsed:   31.9s remaining:   47.8s\n",
      "[Parallel(n_jobs=20)]: Done  21 out of  40 | elapsed:   34.3s remaining:   31.0s\n",
      "[Parallel(n_jobs=20)]: Done  26 out of  40 | elapsed:   37.3s remaining:   20.1s\n",
      "[Parallel(n_jobs=20)]: Done  31 out of  40 | elapsed:   40.5s remaining:   11.8s\n",
      "[Parallel(n_jobs=20)]: Done  36 out of  40 | elapsed:   43.5s remaining:    4.8s\n",
      "[Parallel(n_jobs=20)]: Done  40 out of  40 | elapsed:   48.3s finished\n"
     ]
    }
   ],
   "source": [
    "group_dfs_rle = apply_parallel_joblib(\n",
    "    feedback_df.groupby([\"cohort\", \"mouse_id\"], as_index=False),\n",
    "    _func_rle,\n",
    "    n_jobs=20,\n",
    "    verbose=10,\n",
    ")\n",
    "group_dfs_nonrle = apply_parallel_joblib(\n",
    "    feedback_df.groupby([\"cohort\", \"mouse_id\"], as_index=False),\n",
    "    _func_nonrle,\n",
    "    n_jobs=20,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dfs_rle[\"rle\"] = True\n",
    "group_dfs_nonrle[\"rle\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_dfs = pd.concat([group_dfs_rle, group_dfs_nonrle])\n",
    "group_dfs.index = range(len(group_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = group_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load in exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = norm_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df.loc[use_df[\"session_number\"].isin([-1, 0]), \"session_type\"] = \"pre\"\n",
    "use_df.loc[use_df[\"session_number\"].isin([1, 2]), \"session_type\"] = \"stim\"\n",
    "use_df.loc[use_df[\"session_number\"].isin([3, 4]), \"session_type\"] = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = use_df[(use_df[\"bin_end\"] - use_df[\"bin_start\"]) == bin_size].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df[\"syllable_group\"] = (\n",
    "    use_df.groupby([\"mouse_id\", \"cohort\"])[\"syllable_group\"].rank(method=\"dense\") - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = use_df.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "    use_df[\"log2_fold_change_count\"] = np.log2(use_df[\"fold_change_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.factorize(\n",
    "    pd._libs.lib.fast_zip(\n",
    "        [\n",
    "            use_df[\"mouse_id\"].values,\n",
    "            use_df[\"stim_duration\"].values,\n",
    "            use_df[\"syllable_group\"].values,\n",
    "            use_df[\"target_syllable\"].values,\n",
    "            use_df[\"cohort\"].values,\n",
    "        ]\n",
    "    )\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df[\"syllable_group_unique\"] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df[\"syllable\"] = use_df[\"syllable\"].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df.to_parquet(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/markowitzmeister_gmail_com/jeff_win_share/reinforcement_data/_final_test/_data/optoda_raw_data/learning_timecourse_binsize-30.parquet\n"
     ]
    }
   ],
   "source": [
    "print(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "area        opsin   \n",
       "ctrl        ctrl        12\n",
       "snc (axon)  chr2        20\n",
       "            chrimson     8\n",
       "Name: mouse_id, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_df.groupby([\"area\",\"opsin\"])[\"mouse_id\"].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spont-da]",
   "language": "python",
   "name": "conda-env-spont-da-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
